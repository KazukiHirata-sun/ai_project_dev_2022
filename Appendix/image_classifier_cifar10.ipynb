{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNBrnSIsQGVIhsHHUfuqbEA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KazukiHirata-sun/ai_project_dev_2022/blob/main/Appendix/image_classifier_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training and preservation\n",
        "Build and train a [CNN model](https://viblo.asia/p/deep-learning-tim-hieu-ve-mang-tich-chap-cnn-maGK73bOKj2) for [image identification](https://www.analyticsvidhya.com/blog/2020/02/learn-image-classification-cnn-convolutional-neural-networks-3-datasets/) at Google Colaboratory.  \n",
        "In this case, we will use [cifar-10](https://www.cs.toronto.edu/~kriz/cifar.html) as training data.  \n",
        "The trained model is saved and downloaded.\n",
        "\n",
        "[Refarence](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)"
      ],
      "metadata": {
        "id": "rsFx1jT2pFhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "Load CIFAR-10 and configure DataLoader settings along with data expansion.  "
      ],
      "metadata": {
        "id": "a6Fe5fLXpoeW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlqygVNio7BI"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import CIFAR10\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "affine = transforms.RandomAffine([-15, 15], scale=(0.8, 1.2))  # Rotate and Resize\n",
        "flip = transforms.RandomHorizontalFlip(p=0.5)  # right side flip\n",
        "normalize = transforms.Normalize((0.0, 0.0, 0.0), (1.0, 1.0, 1.0))  # Mean is 0, standard deviation is 1\n",
        "to_tensor = transforms.ToTensor()\n",
        "\n",
        "transform_train = transforms.Compose([affine, flip, to_tensor, normalize])\n",
        "transform_test = transforms.Compose([to_tensor, normalize])\n",
        "cifar10_train = CIFAR10(\"./data\", train=True, download=True, transform=transform_train)\n",
        "cifar10_test = CIFAR10(\"./data\", train=False, download=True, transform=transform_test)\n",
        "\n",
        "# DataLoader settings\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(cifar10_train, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(cifar10_test, batch_size=len(cifar10_test), shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixed seeds"
      ],
      "metadata": {
        "id": "HGEXxVT5DQFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Setup random seed\n",
        "def set_seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "set_seed_everything(2022)"
      ],
      "metadata": {
        "id": "L8nTMSIfPQkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building\n",
        "Build a model of CNN as a class that inherits from the [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) module."
      ],
      "metadata": {
        "id": "9byMWX5frQL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)  # Convolutional layer: (number of input channels, number of filters, filter size)\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # Pooling layer: (area size, stride)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16*5*5, 256)  # Fully Connected layer\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16*5*5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "net.cuda()  # GPU\n",
        "print(net)"
      ],
      "metadata": {
        "id": "p4VAa8UTpFFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning\n",
        "Train the model.  \n",
        "Set the cross-entropy error as the [loss function](https://www.datarobot.com/blog/introduction-to-loss-functions/#:~:text=Further%20reading-,What's%20a%20loss%20function%3F,ll%20output%20a%20lower%20number.) and Adam as the [optimization algorithm](https://d2l.ai/chapter_optimization/index.html). \n",
        "Training takes time, so select GPU in Edit â†’ Hardware Accelerator in Notebook Settings."
      ],
      "metadata": {
        "id": "oYpZJkCnsAbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "# Cross Entropy\n",
        "loss_fnc = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimization Algorithm\n",
        "optimizer = optim.Adam(net.parameters())\n",
        "\n",
        "record_loss_train = []\n",
        "record_loss_test = []\n",
        "\n",
        "# Leaning\n",
        "x_test, t_test = iter(test_loader).next()\n",
        "x_test, t_test = x_test.cuda(), t_test.cuda()\n",
        "for i in range(20):  # 20 epoc\n",
        "    net.train()  # Train\n",
        "    loss_train = 0\n",
        "    for j, (x, t) in enumerate(train_loader):  # Mini Batch\n",
        "        x, t = x.cuda(), t.cuda()  # GPU\n",
        "        y = net(x)\n",
        "        loss = loss_fnc(y, t)\n",
        "        loss_train += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    loss_train /= j+1\n",
        "    record_loss_train.append(loss_train)\n",
        "\n",
        "    net.eval()  # evalation \n",
        "    y_test = net(x_test)\n",
        "    loss_test = loss_fnc(y_test, t_test).item()\n",
        "    record_loss_test.append(loss_test)\n",
        "\n",
        "    if i%1 == 0:\n",
        "        print(\"Epoch:\", i, \"Loss_Train:\", loss_train, \"Loss_Test:\", loss_test)"
      ],
      "metadata": {
        "id": "nVYfjfIvsLnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Error Trends\n",
        "Displays graphs of error trends in training and test data.\n",
        "\n",
        "[Refarence](https://matplotlib.org/) "
      ],
      "metadata": {
        "id": "BZ6qMiR4t7Uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(len(record_loss_train)), record_loss_train, label=\"Train\")\n",
        "plt.plot(range(len(record_loss_test)), record_loss_test, label=\"Test\")\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Jkzo38JduB_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy\n",
        "To understand the performance of the model, we measure the percentage of correct answers using test data."
      ],
      "metadata": {
        "id": "uQOFO2KaubyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "net.eval()  # Evaluation\n",
        "for i, (x, t) in enumerate(test_loader):\n",
        "    x, t = x.cuda(), t.cuda()  # GPU\n",
        "    y = net(x)\n",
        "    correct += (y.argmax(1) == t).sum().item()\n",
        "    total += len(x)\n",
        "print(\"Accuracy:\", str(correct/total*100) + \"%\")"
      ],
      "metadata": {
        "id": "koU67E_LuuFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save model\n",
        "Save the parameters of the trained model.    \n",
        "Each parameter of the model can be obtained by `state_dict()`, so save it."
      ],
      "metadata": {
        "id": "P-_DM5hxzG-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "for key in net.state_dict():\n",
        "    print(key, \": \", net.state_dict()[key].size())\n",
        "print(net.state_dict()[\"conv1.weight\"][0]) \n",
        "\n",
        "# Save Model\n",
        "torch.save(net.state_dict(), \"model_cnn.pth\")  "
      ],
      "metadata": {
        "id": "vKe7fCiAu9UD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}