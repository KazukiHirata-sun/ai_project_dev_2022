{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1BFPk7u2iqa_6PBlUmBszVKZyLR1rYVA2",
      "authorship_tag": "ABX9TyO7jmpZ9X9rz+3ctKSN38fe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KazukiHirata-sun/ai_project_dev_2022/blob/main/section_2/BERT_Fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification of Japanese Sentences with BERT\n",
        "[Fine-tune BERT's model](https://towardsdatascience.com/what-exactly-happens-when-we-fine-tune-bert-f5dc32885d76) on the Japanese dataset to classify the news."
      ],
      "metadata": {
        "id": "yY3wzWSB3XIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation of all library we need\n"
      ],
      "metadata": {
        "id": "bR4brJEk5reb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RPsaxwUx2uiu"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install nlp\n",
        "!pip install datasets\n",
        "!pip install fugashi\n",
        "!pip install ipadic"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting with Google Drive\n",
        "Mount our Google Drive using the authorization code."
      ],
      "metadata": {
        "id": "ZJCyMomQ6C_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "metadata": {
        "id": "WaQUMFZK6OWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up your own working folder\n",
        "workFolder = \"/content/drive/MyDrive/bert_nlp/\""
      ],
      "metadata": {
        "id": "kEl0ogf_by_B"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Dataset\n",
        "Load a [news dataset](https://www.rondhuit.com/download.html#ldcc) stored on Google Drive.\n",
        "\n"
      ],
      "metadata": {
        "id": "nNUT860O6Qlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "raw_data_path = workFolder + \"text/\"\n",
        "\n",
        "dir_files = os.listdir(path=raw_data_path)\n",
        "dirs = [f for f in dir_files if os.path.isdir(os.path.join(raw_data_path, f))] \n",
        "\n",
        "text_label_data = []\n",
        "dir_count = 0 \n",
        "file_count= 0 \n",
        "\n",
        "for i in range(len(dirs)):\n",
        "    dir = dirs[i]\n",
        "    files = glob.glob(raw_data_path + dir + \"/*.txt\") \n",
        "    dir_count += 1\n",
        "\n",
        "    for file in files:\n",
        "        if os.path.basename(file) == \"LICENSE.txt\":\n",
        "            continue\n",
        "\n",
        "        with open(file, \"r\") as f:\n",
        "            text = f.readlines()[3:]\n",
        "            text = \"\".join(text)\n",
        "            text = text.translate(str.maketrans({\"\\n\":\"\", \"\\t\":\"\", \"\\r\":\"\", \"\\u3000\":\"\"})) \n",
        "            text_label_data.append([text, i])\n",
        "\n",
        "        file_count += 1\n",
        "        print(\"\\rfiles: \" + str(file_count) + \" dirs: \" + str(dir_count), end=\"\")"
      ],
      "metadata": {
        "id": "321N4Unb7HTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Data\n",
        "Devide the data into training and test data and save them as csv files to Google Drive."
      ],
      "metadata": {
        "id": "eMHBmUcw7Q_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split for training and testing data\n",
        "news_train, news_test =  train_test_split(text_label_data, shuffle=True)\n",
        "data_path = workFolder + \"data/\"\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "    os.makedirs(data_path)\n",
        "\n",
        "with open(data_path+\"news_train.csv\", \"w\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(news_train)\n",
        "\n",
        "with open(data_path+\"news_test.csv\", \"w\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(news_test)"
      ],
      "metadata": {
        "id": "30LrBdIR7Y5S"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Models and Tokenizers\n",
        "Load a pre-trained Japanese model and its associated Tokenizer."
      ],
      "metadata": {
        "id": "ITQeojCQ7cRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, BertJapaneseTokenizer\n",
        "\n",
        "model_name ='cl-tohoku/bert-base-japanese-whole-word-masking'\n",
        "\n",
        "sc_model = BertForSequenceClassification.from_pretrained(model_name, num_labels=9)\n",
        "sc_model.cuda()\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "G1EZTRSr7jbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data Sets\n",
        "Loads stored news data."
      ],
      "metadata": {
        "id": "Tc9aABUT7is6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=128)\n",
        "    \n",
        "data_path = workFolder + \"data/\"\n",
        "\n",
        "train_data = load_dataset(\"csv\", data_files=data_path+\"news_train.csv\", column_names=[\"text\", \"label\"], split=\"train\")\n",
        "train_data = train_data.map(tokenize, batched=True, batch_size=len(train_data))\n",
        "train_data.set_format(\"torch\", columns=[\"input_ids\", \"label\"])\n",
        "\n",
        "test_data = load_dataset(\"csv\", data_files=data_path+\"news_test.csv\", column_names=[\"text\", \"label\"], split=\"train\")\n",
        "test_data = test_data.map(tokenize, batched=True, batch_size=len(test_data))\n",
        "test_data.set_format(\"torch\", columns=[\"input_ids\", \"label\"])"
      ],
      "metadata": {
        "id": "Ht4DsZkQ7i6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions for evaluation\n",
        "Use `sklearn.metrics()` to define functions for evaluating models."
      ],
      "metadata": {
        "id": "e_d1sDqo7wgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(result):\n",
        "    labels = result.label_ids\n",
        "    preds = result.predictions.argmax(-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "    }"
      ],
      "metadata": {
        "id": "vFYD1_4S72tT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up a Trainer\n",
        "Use the [Trainer](https://huggingface.co/transformers/main_classes/trainer.html) and [TrainingArguments](https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments) classes to set up a Trainer to train. \n"
      ],
      "metadata": {
        "id": "ag-OvpyG8CsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"./results\",\n",
        "    num_train_epochs = 2,\n",
        "    per_device_train_batch_size = 8,\n",
        "    per_device_eval_batch_size = 32,\n",
        "    warmup_steps = 500, \n",
        "    weight_decay = 0.01,\n",
        "    logging_dir = \"./logs\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = sc_model,\n",
        "    args = training_args,\n",
        "    compute_metrics = compute_metrics,\n",
        "    train_dataset = train_data,\n",
        "    eval_dataset = test_data,\n",
        ")"
      ],
      "metadata": {
        "id": "WXLuYJ4A8AnT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training\n",
        "Fine tuning based on the setting."
      ],
      "metadata": {
        "id": "_kk11jqY8VcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "EByvLv3b8YvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Models\n",
        "The trainer's `evaluate()` method evaluates the model."
      ],
      "metadata": {
        "id": "DxDw13Y98Z-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "jIa0Ppzx8ead"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model\n",
        "Saves a trained model."
      ],
      "metadata": {
        "id": "OzXNkH4K82IR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = workFolder + \"model/\"\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "trainer.save_model(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ],
      "metadata": {
        "id": "EkgcdD-l87uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading a model\n",
        "Loads a previously saved model."
      ],
      "metadata": {
        "id": "lcOx1LSn9GXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "loaded_model.cuda()\n",
        "loaded_tokenizer = BertJapaneseTokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "4fbdm9FD9LIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Japanese News Classification\n",
        "Classify news using the loaded model."
      ],
      "metadata": {
        "id": "8xwnKylN9O-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "# Loading the data to be classified.\n",
        "file = raw_data_path + \"/sports-watch/sports-watch-4764756.txt\"\n",
        "with open(file, \"r\") as f:\n",
        "    sample_text = f.readlines()[3:]\n",
        "    sample_text = \"\".join(sample_text)\n",
        "    sample_text = sample_text.translate(str.maketrans({\"\\n\":\"\", \"\\t\":\"\", \"\\r\":\"\", \"\\u3000\":\"\"})) \n",
        "\n",
        "# # https://www.infoq.com/jp/articles/ai-devops-takeover/?itm_source=articles_about_ai-ml-data-eng&itm_medium=link&itm_campaign=ai-ml-data-eng\n",
        "# sample_text = \"開発者の多くにとって、DevOpsの次に何が来るかを予測することは、ある種の気晴らしになっています。この10年間、私たちは、私たちの業界が急速に変化するのを目の当たりにしてきました。その間には、プログラマの役割も根本から変わってきています。\"\n",
        "\n",
        "max_length = 512\n",
        "words = loaded_tokenizer.tokenize(sample_text)\n",
        "word_ids = loaded_tokenizer.convert_tokens_to_ids(words)\n",
        "word_tensor = torch.tensor([word_ids[:max_length]])\n",
        "\n",
        "# Prediction\n",
        "x = word_tensor.cuda()  \n",
        "y = loaded_model(x)\n",
        "pred = y[0].argmax(-1) \n",
        "\n",
        "# Displaying the results\n",
        "raw_data_path = workFolder + \"/text/\"\n",
        "dir_files = os.listdir(path=raw_data_path)\n",
        "dirs = [f for f in dir_files if os.path.isdir(os.path.join(raw_data_path, f))]\n",
        "print(\"結果は\", dirs[pred])"
      ],
      "metadata": {
        "id": "7BJoasNg9W8W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}